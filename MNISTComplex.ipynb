{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 14.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 547kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.87MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.87MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot\n",
    "import torch.nn as nn\n",
    "from numpy import fft\n",
    "\n",
    "#Preprocessing real and complex values\n",
    "'''\n",
    "__call__ turns this obejct into a function\n",
    "__init__ is the equivalent of a constructor in Java\n",
    "'''\n",
    "class FFTTransform:\n",
    "    def __call__(self,x):\n",
    "        x = x.squeeze(0)\n",
    "        x_fft = torch.fft.fft2(x)\n",
    "        return x_fft\n",
    "\n",
    "base_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "complex_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    FFTTransform()\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#Real Values\n",
    "train_dataset = datasets.MNIST(root='./data',download=True, train=True, transform=base_transforms)\n",
    "test_dataset  = datasets.MNIST(root='./data',download=True, train=False, transform=base_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "#Complex Values\n",
    "train_dataset_i = datasets.MNIST(root='./data',download=True, train=True, transform =complex_transforms)\n",
    "test_dataset_i = datasets.MNIST(root='./data',download=True, train=False, transform=complex_transforms)\n",
    "\n",
    "train_loader_i = DataLoader(train_dataset_i, batch_size=batch_size, shuffle=True)\n",
    "test_loader_i = DataLoader(test_dataset_i, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#Real valued data\n",
    "class RealMLP(nn.Module):\n",
    "    def __init__(self, bias=True):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(784, 128),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),   \n",
    "        )\n",
    "#This final output is a tensor of logits\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "'''\n",
    "Imaginary Valued Data\n",
    "\n",
    "Consists of two parts:\n",
    "1. Building a custom linear dense layer that can accommodate\n",
    "imaginary values\n",
    "2. Building model architecture akin to RealMLP, but with custom dense layer \n",
    "instead of nn.Linear(x,y)\n",
    "\n",
    "There are two additional areas to explore: \n",
    "1. Experiment with handling the bias function differently\n",
    "2. Experiment with different complex activation functions \n",
    "'''\n",
    "\n",
    "\n",
    "class ComplexLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.real_weight = nn.Linear(in_features, out_features)\n",
    "        self.complex_weight = nn.Linear(in_features, out_features)\n",
    "\n",
    "#Ensure x is a torch.complex \n",
    "    def forward(self, x):\n",
    "        x_real = x.real\n",
    "        x_imag = x.imag\n",
    "\n",
    "        real_out = self.real_weight(x_real) - self.complex_weight(x_imag)\n",
    "        imag_out = self.real_weight(x_imag) + self.complex_weight(x_real)\n",
    "\n",
    "        return torch.complex(real_out, imag_out)\n",
    "\n",
    "\n",
    "class ComplexMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = ComplexLinear(784, 128)\n",
    "        self.fc2 = ComplexLinear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x.real) + 1j * torch.relu(x.imag)  \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=558.9601, Accuracy=91.73%\n",
      "Epoch 2: Loss=242.9819, Accuracy=96.18%\n",
      "Epoch 3: Loss=167.1954, Accuracy=97.33%\n",
      "Epoch 4: Loss=122.9449, Accuracy=98.02%\n",
      "Epoch 5: Loss=97.7867, Accuracy=98.41%\n",
      "Epoch 6: Loss=75.0241, Accuracy=98.78%\n",
      "Epoch 7: Loss=62.5222, Accuracy=98.94%\n",
      "Epoch 8: Loss=49.4900, Accuracy=99.21%\n",
      "Epoch 9: Loss=39.8574, Accuracy=99.32%\n",
      "Epoch 10: Loss=33.1670, Accuracy=99.45%\n",
      "Epoch 10, Train Loss: 0.0318, Val Accuracy: 97.90%\n"
     ]
    }
   ],
   "source": [
    "model = RealMLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.reshape(batch_size, x_batch.shape[2]*x_batch.shape[3])\n",
    "        preds = model(x_batch)\n",
    "        \n",
    "        loss = criterion(preds, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (preds.argmax(dim=1) == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    acc = correct / total * 100\n",
    "    print(f\"Epoch {epoch+1}: Loss={total_loss:.4f}, Accuracy={acc:.2f}%\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch = x_batch.view(x_batch.size(0), -1)\n",
    "            outputs = model(x_batch)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    acc = correct / total * 100\n",
    "    return acc\n",
    "\n",
    "val_acc = evaluate(model, test_loader)\n",
    "print(f\"Epoch {epoch+1}, Train Loss: {loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=440.9363, Accuracy=93.23%\n",
      "Epoch 2: Loss=196.9401, Accuracy=96.88%\n",
      "Epoch 3: Loss=153.0192, Accuracy=97.69%\n",
      "Epoch 4: Loss=130.6195, Accuracy=98.03%\n",
      "Epoch 5: Loss=106.2810, Accuracy=98.47%\n",
      "Epoch 6: Loss=113.8958, Accuracy=98.56%\n",
      "Epoch 7: Loss=103.0648, Accuracy=98.71%\n",
      "Epoch 8: Loss=90.0500, Accuracy=98.95%\n",
      "Epoch 9: Loss=82.7339, Accuracy=99.03%\n",
      "Epoch 10: Loss=86.4420, Accuracy=99.08%\n",
      "Epoch 10, Train Loss: 86.4420, Val Accuracy: 96.43%\n"
     ]
    }
   ],
   "source": [
    "#Complex Model 1\n",
    "'''Experimenting with bias:\n",
    "\n",
    "CVNN\n",
    "If bias = True, accuracy ~ 97.02\n",
    "If bias = False, accuracy ~97.00\n",
    "\n",
    "If bias = True, accuracy ~ 97.90\n",
    "If bias = False, accuracy ~97.83\n",
    "\n",
    "Negligble across models/dataset\n",
    "'''\n",
    "complex_model = ComplexMLP()\n",
    "complex_criterion = nn.CrossEntropyLoss()\n",
    "complex_optimizer = torch.optim.Adam(complex_model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x_batch, y_batch in train_loader_i:\n",
    "        x_batch = x_batch.reshape(batch_size, x_batch.shape[1]*x_batch.shape[2])\n",
    "        preds = complex_model(x_batch)\n",
    "        preds = preds.real\n",
    "                \n",
    "        complex_loss = complex_criterion(preds, y_batch)\n",
    "\n",
    "        complex_optimizer.zero_grad()\n",
    "        complex_loss.backward()\n",
    "        complex_optimizer.step()\n",
    "\n",
    "        total_loss += complex_loss.item()\n",
    "        correct += (preds.argmax(dim=1) == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    acc = correct / total * 100\n",
    "    print(f\"Epoch {epoch+1}: Loss={total_loss:.4f}, Accuracy={acc:.2f}%\")\n",
    "\n",
    "\n",
    "def imaginary_evaluate(model, test_loader):\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch = x_batch.view(x_batch.size(0), -1)\n",
    "            outputs = model(x_batch)\n",
    "            outputs = outputs.real\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    acc = correct / total * 100\n",
    "    return acc\n",
    "\n",
    "val_acc = imaginary_evaluate(complex_model, test_loader_i)\n",
    "print(f\"Epoch {epoch+1}, Train Loss: {total_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
